{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "class FraudDetectionEnsemble(PythonModel):\n",
    "    def __init__(self, model_versions, data_directory):\n",
    "        \"\"\"\n",
    "        Initialize with model versions and directory containing transaction CSV files\n",
    "        \n",
    "        Args:\n",
    "            model_versions (dict): Model version mapping\n",
    "            data_directory (str): Path to directory containing transaction CSV files\n",
    "        \"\"\"\n",
    "        self.model_versions = model_versions\n",
    "        self.data_directory = data_directory\n",
    "        self.xgb_model = None\n",
    "        self.rf_model = None\n",
    "        self.nn_model = None\n",
    "        self.feature_names = None\n",
    "        self.weights = [0.4, 0.3, 0.3]\n",
    "        \n",
    "    def _load_customer_transactions(self, customer_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Efficiently load historical transactions for a specific customer\n",
    "        \"\"\"\n",
    "        customer_data = []\n",
    "        required_columns = ['CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_DATETIME', \n",
    "                          'TX_TIME_SECONDS', 'TX_TIME_DAYS']\n",
    "        \n",
    "        # Iterate through CSV files in directory\n",
    "        for filename in os.listdir(self.data_directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.data_directory, filename)\n",
    "                \n",
    "                # Read only required columns and filter for customer_id\n",
    "                try:\n",
    "                    # First, check if customer_id exists in this file\n",
    "                    customer_check = pd.read_csv(\n",
    "                        file_path, \n",
    "                        usecols=['CUSTOMER_ID'], \n",
    "                        dtype={'CUSTOMER_ID': int}\n",
    "                    )\n",
    "                    \n",
    "                    if customer_id in customer_check['CUSTOMER_ID'].values:\n",
    "                        # Read only required columns with appropriate dtypes\n",
    "                        df = pd.read_csv(\n",
    "                            file_path,\n",
    "                            usecols=required_columns,\n",
    "                            dtype={\n",
    "                                'CUSTOMER_ID': int,\n",
    "                                'TERMINAL_ID': int,\n",
    "                                'TX_AMOUNT': float,\n",
    "                                'TX_TIME_SECONDS': float,\n",
    "                                'TX_TIME_DAYS': float\n",
    "                            },\n",
    "                            parse_dates=['TX_DATETIME']\n",
    "                        )\n",
    "                        \n",
    "                        # Filter for specific customer\n",
    "                        customer_df = df[df['CUSTOMER_ID'] == customer_id]\n",
    "                        \n",
    "                        if not customer_df.empty:\n",
    "                            customer_data.append(customer_df)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {filename}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not customer_data:\n",
    "            # Return empty DataFrame with correct columns if no data found\n",
    "            return pd.DataFrame(columns=required_columns)\n",
    "        \n",
    "        # Combine all customer data and sort by datetime\n",
    "        customer_history = pd.concat(customer_data, ignore_index=True)\n",
    "        return customer_history.sort_values('TX_DATETIME')\n",
    "\n",
    "    def _calculate_customer_amount_features(self, customer_txns: pd.DataFrame, \n",
    "                                         current_amount: float,\n",
    "                                         tx_datetime: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Calculate amount features based on customer's transaction history\"\"\"\n",
    "        # Filter for transactions before current timestamp\n",
    "        past_txns = customer_txns[customer_txns['TX_DATETIME'] < tx_datetime]\n",
    "        \n",
    "        if past_txns.empty:\n",
    "            return {\n",
    "                'amount': current_amount,\n",
    "                'amount_log': np.log1p(current_amount),\n",
    "                'amount_rounded': round(current_amount, -1),\n",
    "                'is_round_amount': 1 if current_amount % 10 == 0 else 0,\n",
    "                'amount_mean': current_amount,\n",
    "                'amount_std': 0.1 * current_amount,\n",
    "                'amount_max': current_amount,\n",
    "                'amount_min': current_amount,\n",
    "                'amount_deviation': 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate amount statistics\n",
    "        amount_stats = past_txns['TX_AMOUNT'].agg(['mean', 'std', 'max', 'min'])\n",
    "        \n",
    "        # Handle std=0 case\n",
    "        if amount_stats['std'] == 0:\n",
    "            amount_stats['std'] = 0.1 * amount_stats['mean']\n",
    "            \n",
    "        amount_deviation = abs(current_amount - amount_stats['mean']) / amount_stats['std']\n",
    "        \n",
    "        return {\n",
    "            'amount': current_amount,\n",
    "            'amount_log': np.log1p(current_amount),\n",
    "            'amount_rounded': round(current_amount, -1),\n",
    "            'is_round_amount': 1 if current_amount % 10 == 0 else 0,\n",
    "            'amount_mean': amount_stats['mean'],\n",
    "            'amount_std': amount_stats['std'],\n",
    "            'amount_max': amount_stats['max'],\n",
    "            'amount_min': amount_stats['min'],\n",
    "            'amount_deviation': amount_deviation\n",
    "        }\n",
    "\n",
    "    def _calculate_sequence_features(self, customer_txns: pd.DataFrame,\n",
    "                                  terminal_id: int, amount: float,\n",
    "                                  tx_datetime: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Calculate sequence features from customer transaction history\"\"\"\n",
    "        past_txns = customer_txns[customer_txns['TX_DATETIME'] < tx_datetime]\n",
    "        \n",
    "        if past_txns.empty:\n",
    "            return {\n",
    "                'time_since_last': 86400,\n",
    "                'time_until_next': 86400,\n",
    "                'amount_diff_last': 0,\n",
    "                'amount_diff_next': 0,\n",
    "                'terminal_changed': 1,\n",
    "                'tx_velocity_1h': 0,\n",
    "                'tx_velocity_24h': 0,\n",
    "                'amount_velocity_1h': 0,\n",
    "                'amount_velocity_24h': 0,\n",
    "                'unique_terminals_24h': 0,\n",
    "                'repeated_terminal': 0\n",
    "            }\n",
    "        \n",
    "        # Time windows\n",
    "        one_hour_ago = tx_datetime - timedelta(hours=1)\n",
    "        one_day_ago = tx_datetime - timedelta(days=1)\n",
    "        \n",
    "        # Get last transaction\n",
    "        last_tx = past_txns.iloc[-1]\n",
    "        \n",
    "        # Calculate time-based features\n",
    "        time_since_last = (tx_datetime - last_tx['TX_DATETIME']).total_seconds()\n",
    "        amount_diff = amount - last_tx['TX_AMOUNT']\n",
    "        terminal_changed = 1 if last_tx['TERMINAL_ID'] != terminal_id else 0\n",
    "        \n",
    "        # Calculate velocity features using vectorized operations\n",
    "        mask_1h = (past_txns['TX_DATETIME'] > one_hour_ago)\n",
    "        mask_24h = (past_txns['TX_DATETIME'] > one_day_ago)\n",
    "        \n",
    "        txns_1h = past_txns[mask_1h]\n",
    "        txns_24h = past_txns[mask_24h]\n",
    "        \n",
    "        return {\n",
    "            'time_since_last': time_since_last,\n",
    "            'time_until_next': 0,\n",
    "            'amount_diff_last': amount_diff,\n",
    "            'amount_diff_next': 0,\n",
    "            'terminal_changed': terminal_changed,\n",
    "            'tx_velocity_1h': len(txns_1h),\n",
    "            'tx_velocity_24h': len(txns_24h),\n",
    "            'amount_velocity_1h': txns_1h['TX_AMOUNT'].sum(),\n",
    "            'amount_velocity_24h': txns_24h['TX_AMOUNT'].sum(),\n",
    "            'unique_terminals_24h': txns_24h['TERMINAL_ID'].nunique(),\n",
    "            'repeated_terminal': past_txns[past_txns['TERMINAL_ID'] == terminal_id].shape[0]\n",
    "        }\n",
    "\n",
    "    def _preprocess_input(self, data):\n",
    "        \"\"\"Preprocess input with efficient customer-level feature calculation\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            if 'inputs' in data:\n",
    "                data = data['inputs']\n",
    "            if isinstance(data, dict):\n",
    "                data = pd.DataFrame([data])\n",
    "        \n",
    "        # Extract base values\n",
    "        customer_id = int(data['CUSTOMER_ID'].iloc[0])\n",
    "        terminal_id = int(data['TERMINAL_ID'].iloc[0])\n",
    "        amount = float(data['TX_AMOUNT'].iloc[0])\n",
    "        tx_datetime = pd.to_datetime(data['TX_DATETIME'].iloc[0])\n",
    "        \n",
    "        # Load customer's transaction history\n",
    "        customer_txns = self._load_customer_transactions(customer_id)\n",
    "        \n",
    "        # Calculate features using the loaded history\n",
    "        amount_features = self._calculate_customer_amount_features(\n",
    "            customer_txns, amount, tx_datetime\n",
    "        )\n",
    "        \n",
    "        sequence_features = self._calculate_sequence_features(\n",
    "            customer_txns, terminal_id, amount, tx_datetime\n",
    "        )\n",
    "        \n",
    "        # Create features DataFrame\n",
    "        features = pd.DataFrame(index=[0])\n",
    "        \n",
    "        # Map all features\n",
    "        features['feature_1'] = customer_id / 10000\n",
    "        features['feature_2'] = float(data['TX_TIME_SECONDS'].iloc[0]) / 86400\n",
    "        features['feature_3'] = float(data['TX_TIME_DAYS'].iloc[0]) / 7\n",
    "        features['feature_4'] = terminal_id / 1000\n",
    "        features['feature_5'] = amount_features['amount']\n",
    "        features['feature_6'] = amount_features['amount_log']\n",
    "        \n",
    "        # Temporal features\n",
    "        features['feature_7'] = tx_datetime.hour / 24\n",
    "        features['feature_8'] = tx_datetime.dayofweek / 7\n",
    "        features['feature_9'] = tx_datetime.month / 12\n",
    "        features['feature_10'] = 1 if tx_datetime.dayofweek >= 5 else 0\n",
    "        features['feature_11'] = 1 if (tx_datetime.hour >= 23 or tx_datetime.hour <= 4) else 0\n",
    "        features['feature_12'] = 1 if (8 <= tx_datetime.hour <= 10 or \n",
    "                                     16 <= tx_datetime.hour <= 18) else 0\n",
    "        \n",
    "        # Amount features\n",
    "        features['feature_13'] = amount_features['amount_deviation']\n",
    "        features['feature_14'] = amount_features['amount_mean']\n",
    "        features['feature_15'] = amount_features['amount_std']\n",
    "        features['feature_16'] = amount_features['amount_max']\n",
    "        features['feature_17'] = amount_features['amount_min']\n",
    "        \n",
    "        # Customer behavior features\n",
    "        features['feature_18'] = len(customer_txns)\n",
    "        features['feature_19'] = customer_txns['TERMINAL_ID'].nunique()\n",
    "        features['feature_20'] = customer_txns['TX_DATETIME'].dt.hour.mean()\n",
    "        features['feature_21'] = customer_txns['TX_DATETIME'].dt.hour.std()\n",
    "        \n",
    "        # Terminal features (placeholder for now)\n",
    "        for i in range(22, 30):\n",
    "            features[f'feature_{i}'] = 0.0\n",
    "        \n",
    "        # Sequence features\n",
    "        features['feature_30'] = sequence_features['time_since_last']\n",
    "        features['feature_31'] = sequence_features['time_until_next']\n",
    "        features['feature_32'] = sequence_features['amount_diff_last']\n",
    "        features['feature_33'] = sequence_features['amount_diff_next']\n",
    "        features['feature_34'] = sequence_features['terminal_changed']\n",
    "        features['feature_35'] = sequence_features['tx_velocity_1h']\n",
    "        features['feature_36'] = sequence_features['tx_velocity_24h']\n",
    "        features['feature_37'] = sequence_features['repeated_terminal']\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load models\"\"\"\n",
    "        if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "            mlflow.set_tracking_uri(\"databricks\")\n",
    "        else:\n",
    "            mlflow.set_tracking_uri(\"local\")\n",
    "            \n",
    "        self.xgb_model = self._load_model(context, 'xgb_model', self.model_versions['xgb_model'])\n",
    "        self.rf_model = self._load_model(context, 'rf_model', self.model_versions['rf_model'])\n",
    "        self.nn_model = self._load_model(context, 'pytorch_model', self.model_versions['pytorch_model'])\n",
    "        \n",
    "        self.feature_names = [f'feature_{i}' for i in range(1, 38)]\n",
    "\n",
    "    def _load_model(self, workspace, model_name, version):\n",
    "        return mlflow.pyfunc.load_model(\n",
    "            model_uri=f\"models:/{model_name}/{version}\"\n",
    "        )\n",
    "\n",
    "    def predict(self, context, input_data):\n",
    "        \"\"\"Make predictions using the ensemble\"\"\"\n",
    "        try:\n",
    "            # Preprocess input data\n",
    "            X = self._preprocess_input(input_data)\n",
    "            \n",
    "            # Get individual model predictions\n",
    "            xgb_prob = self.xgb_model.predict(X)\n",
    "            rf_prob = self.rf_model.predict(X)\n",
    "            nn_prob = self.nn_model.predict(X)\n",
    "            \n",
    "            # Convert predictions to probabilities if needed\n",
    "            if hasattr(xgb_prob, 'iloc'):\n",
    "                xgb_prob = float(xgb_prob.iloc[0])\n",
    "            elif isinstance(xgb_prob, np.ndarray):\n",
    "                xgb_prob = float(xgb_prob[0])\n",
    "            \n",
    "            if hasattr(rf_prob, 'iloc'):\n",
    "                rf_prob = float(rf_prob.iloc[0])\n",
    "            elif isinstance(rf_prob, np.ndarray):\n",
    "                rf_prob = float(rf_prob[0])\n",
    "            \n",
    "            if hasattr(nn_prob, 'iloc'):\n",
    "                nn_prob = float(nn_prob.iloc[0])\n",
    "            elif isinstance(nn_prob, np.ndarray):\n",
    "                nn_prob = float(nn_prob[0])\n",
    "            \n",
    "            # Calculate ensemble probability\n",
    "            ensemble_prob = (\n",
    "                self.weights[0] * xgb_prob +\n",
    "                self.weights[1] * rf_prob +\n",
    "                self.weights[2] * nn_prob\n",
    "            )\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = \"TXN IS FRAUDULENT\" if ensemble_prob >= 0.5 else \"TXN IS NOT FRAUDULENT\"\n",
    "            \n",
    "            return [{\n",
    "                'prediction': prediction,\n",
    "                'probability': ensemble_prob,\n",
    "                'model_predictions': {\n",
    "                    'xgboost': xgb_prob,\n",
    "                    'random_forest': rf_prob,\n",
    "                    'neural_network': nn_prob\n",
    "                },\n",
    "                'ensemble_weights': {\n",
    "                    'xgboost': self.weights[0],\n",
    "                    'random_forest': self.weights[1],\n",
    "                    'neural_network': self.weights[2]\n",
    "                }\n",
    "            }]\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Prediction error: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    serving_payload = {\n",
    "        \"inputs\": {\n",
    "            \"TRANSACTION_ID\": 4781,\n",
    "            \"TX_DATETIME\": \"2024-10-29 05:57:40\",\n",
    "            \"CUSTOMER_ID\": 17085,\n",
    "            \"TERMINAL_ID\": 139,\n",
    "            \"TX_AMOUNT\": 251.25,\n",
    "            \"TX_TIME_SECONDS\": 21460,\n",
    "            \"TX_TIME_DAYS\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_versions = {\n",
    "        'xgb_model': '1',\n",
    "        'rf_model': '1',\n",
    "        'pytorch_model': '1'\n",
    "    }\n",
    "    \n",
    "    model = FraudDetectionEnsemble(\n",
    "        model_versions=model_versions,\n",
    "        data_directory='/path/to/transaction/data'\n",
    "    )\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"fraud_detection_ensemble\",\n",
    "        python_model=model,\n",
    "        registered_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
