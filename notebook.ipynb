{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the three parts\n",
    "import pandas as pd \n",
    "import datetime\n",
    "first_part = pd.read_csv(\"C://ccfrauddetection//data/data_part1.csv\")\n",
    "second_part = pd.read_csv(\"C://ccfrauddetection//data/data_part2.csv\")\n",
    "third_part = pd.read_csv(\"C://ccfrauddetection//data/data_part3.csv\")\n",
    "\n",
    "transactions_df = pd.concat([first_part, second_part, third_part])\n",
    "transactions_df = transactions_df.drop(transactions_df.columns[0], axis=1)\n",
    "\n",
    "# Count the fraud categories\n",
    "fraud_distribution = transactions_df['TX_FRAUD'].value_counts()\n",
    "print(\"Distribution of fraud cases:\")\n",
    "print(fraud_distribution)\n",
    "print(transactions_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_set(transactions_df,\n",
    "                       start_date_training,\n",
    "                       delta_train=7,delta_delay=7,delta_test=7,\n",
    "                       sampling_ratio=1.0,\n",
    "                       random_state=0):\n",
    "    \n",
    "    start_date_training = datetime.datetime.strptime(start_date_training, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Get the training set data\n",
    "    train_df = transactions_df[(transactions_df.TX_DATETIME>=start_date_training) &\n",
    "                               (transactions_df.TX_DATETIME<start_date_training+ datetime.timedelta(days=delta_train))]\n",
    "    \n",
    "    # Get the test set data\n",
    "    test_df = []\n",
    "    \n",
    "    # Note: Cards known to be compromised after the delay period are removed from the test set\n",
    "    # That is, for each test day, all frauds known at (test_day-delay_period) are removed\n",
    "    \n",
    "    # First, get known defrauded customers from the training set\n",
    "    known_defrauded_customers = set(train_df[train_df.TX_FRAUD==1].CUSTOMER_ID)\n",
    "    \n",
    "    # Get the relative starting day of training set (easier than TX_DATETIME to collect test data)\n",
    "    start_tx_time_days_training = train_df.TX_TIME_DAYS.min()\n",
    "    \n",
    "    # Then, for each day of the test set\n",
    "    for day in range(delta_test):\n",
    "    \n",
    "        # Get test data for that day\n",
    "        test_df_day = transactions_df[transactions_df.TX_TIME_DAYS==start_tx_time_days_training+\n",
    "                                                                    delta_train+delta_delay+\n",
    "                                                                    day]\n",
    "        \n",
    "        # Compromised cards from that test day, minus the delay period, are added to the pool of known defrauded customers\n",
    "        test_df_day_delay_period = transactions_df[transactions_df.TX_TIME_DAYS==start_tx_time_days_training+\n",
    "                                                                                delta_train+\n",
    "                                                                                day-1]\n",
    "        \n",
    "        new_defrauded_customers = set(test_df_day_delay_period[test_df_day_delay_period.TX_FRAUD==1].CUSTOMER_ID)\n",
    "        known_defrauded_customers = known_defrauded_customers.union(new_defrauded_customers)\n",
    "        \n",
    "        test_df_day = test_df_day[~test_df_day.CUSTOMER_ID.isin(known_defrauded_customers)]\n",
    "        \n",
    "        test_df.append(test_df_day)\n",
    "        \n",
    "    test_df = pd.concat(test_df)\n",
    "    \n",
    "    # If subsample\n",
    "    if sampling_ratio<1:\n",
    "        \n",
    "        train_df_frauds=train_df[train_df.TX_FRAUD==1].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df_genuine=train_df[train_df.TX_FRAUD==0].sample(frac=sampling_ratio, random_state=random_state)\n",
    "        train_df=pd.concat([train_df_frauds,train_df_genuine])\n",
    "        \n",
    "    # Sort data sets by ascending order of transaction ID\n",
    "    train_df=train_df.sort_values('TRANSACTION_ID')\n",
    "    test_df=test_df.sort_values('TRANSACTION_ID')\n",
    "    \n",
    "    return (train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_train_test_set(transactions_df, start_date_training = transactions_df['TX_DATETIME'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(tx_datetime):\n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    # Binary value: 0 if weekday, 1 if weekend\n",
    "    is_weekend = weekday>=5\n",
    "    return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['TX_DATETIME'] = pd.to_datetime(transactions_df['TX_DATETIME'])\n",
    "transactions_df['TX_DURING_WEEKEND'] = transactions_df.TX_DATETIME.apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(tx_datetime):\n",
    "    \n",
    "    # Get the hour of the transaction\n",
    "    tx_hour = tx_datetime.hour\n",
    "    # Binary value: 1 if hour less than 6, and 0 otherwise\n",
    "    is_night = tx_hour<=6\n",
    "    \n",
    "    return int(is_night)\n",
    "\n",
    "%time transactions_df['TX_DURING_NIGHT']=transactions_df.TX_DATETIME.apply(is_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_spending_behaviour_features(customer_transactions, windows_size_in_days=[1,7,30]):\n",
    "    # Let us first order transactions chronologically\n",
    "    customer_transactions = customer_transactions.sort_values('TX_DATETIME')\n",
    "    \n",
    "    # The transaction date and time is set as the index, which will allow the use of the rolling function\n",
    "    customer_transactions.index = customer_transactions.TX_DATETIME\n",
    "    \n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        # Compute the sum of the transaction amounts and the number of transactions for the given window size\n",
    "        SUM_AMOUNT_TX_WINDOW = customer_transactions['TX_AMOUNT'].rolling(window=window_size, min_periods=1).sum()\n",
    "        NB_TX_WINDOW = customer_transactions['TX_AMOUNT'].rolling(window=window_size, min_periods=1).count()\n",
    "    \n",
    "        # Compute the average transaction amount for the given window size\n",
    "        AVG_AMOUNT_TX_WINDOW = SUM_AMOUNT_TX_WINDOW / NB_TX_WINDOW\n",
    "    \n",
    "        # Save feature values\n",
    "        customer_transactions['CUSTOMER_ID_NB_TX_' + str(window_size) + 'DAY_WINDOW'] = NB_TX_WINDOW\n",
    "        customer_transactions['CUSTOMER_ID_AVG_AMOUNT_' + str(window_size) + 'DAY_WINDOW'] = AVG_AMOUNT_TX_WINDOW\n",
    "    \n",
    "    # Reindex according to transaction IDs\n",
    "    customer_transactions.index = customer_transactions.TRANSACTION_ID\n",
    "        \n",
    "    # Return the dataframe with the new features\n",
    "    return customer_transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_df.groupby('CUSTOMER_ID').apply(lambda x: get_customer_spending_behaviour_features(x, windows_size_in_days=[1,7,30]),include_groups = False)\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_risk_rolling_window(terminal_transactions, delay_period=7, windows_size_in_days=[1,7,30], feature=\"TERMINAL_ID\"):\n",
    "    \n",
    "    terminal_transactions=terminal_transactions.sort_values('TX_DATETIME')\n",
    "    \n",
    "    terminal_transactions.index=terminal_transactions.TX_DATETIME\n",
    "    \n",
    "    NB_FRAUD_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').sum()\n",
    "    NB_TX_DELAY=terminal_transactions['TX_FRAUD'].rolling(str(delay_period)+'d').count()\n",
    "    \n",
    "    for window_size in windows_size_in_days:\n",
    "    \n",
    "        NB_FRAUD_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').sum()\n",
    "        NB_TX_DELAY_WINDOW=terminal_transactions['TX_FRAUD'].rolling(str(delay_period+window_size)+'d').count()\n",
    "    \n",
    "        NB_FRAUD_WINDOW=NB_FRAUD_DELAY_WINDOW-NB_FRAUD_DELAY\n",
    "        NB_TX_WINDOW=NB_TX_DELAY_WINDOW-NB_TX_DELAY\n",
    "    \n",
    "        RISK_WINDOW=NB_FRAUD_WINDOW/NB_TX_WINDOW\n",
    "        \n",
    "        terminal_transactions[feature+'_NB_TX_'+str(window_size)+'DAY_WINDOW']=list(NB_TX_WINDOW)\n",
    "        terminal_transactions[feature+'_RISK_'+str(window_size)+'DAY_WINDOW']=list(RISK_WINDOW)\n",
    "        \n",
    "    terminal_transactions.index=terminal_transactions.TRANSACTION_ID\n",
    "    \n",
    "    # Replace NA values with 0 (all undefined risk scores where NB_TX_WINDOW is 0) \n",
    "    terminal_transactions.fillna(0,inplace=True)\n",
    "    \n",
    "    return terminal_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time transactions_df=transactions_df.groupby('TERMINAL_ID').apply(lambda x: get_count_risk_rolling_window(x, delay_period=7, windows_size_in_days=[1,7,30], feature=\"TERMINAL_ID\"), include_groups = False)\n",
    "transactions_df=transactions_df.sort_values('TX_DATETIME').reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
