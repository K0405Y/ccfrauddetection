{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c427438c-adff-4482-b86a-d561e2966574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from src.preprocessing.prep import TransactionPreprocessor\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "class FraudDetectionEnsemble(PythonModel):\n",
    "    def __init__(self, model_versions, data_directory):\n",
    "        \"\"\"\n",
    "        Initialize with model versions and directory containing transaction CSV files\n",
    "        \n",
    "        Args:\n",
    "            model_versions (dict): Model version mapping\n",
    "            data_directory (str): Path to directory containing transaction CSV files\n",
    "        \"\"\"\n",
    "        self.model_versions = model_versions\n",
    "        self.data_directory = data_directory\n",
    "        self.xgb_model = None\n",
    "        self.rf_model = None\n",
    "        self.nn_model = None\n",
    "        self.feature_names = None\n",
    "        self.weights = [0.4, 0.3, 0.3]\n",
    "        \n",
    "    def _load_customer_transactions(self, customer_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        load historical transactions for a specific customer\n",
    "        \"\"\"\n",
    "        customer_data = []\n",
    "        customer_found = True\n",
    "        required_columns = ['CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_DATETIME', \n",
    "                          'TX_TIME_SECONDS', 'TX_TIME_DAYS']\n",
    "          \n",
    "        # Iterate through txn files in directory\n",
    "        for filename in os.listdir(self.data_directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.data_directory, filename)\n",
    "                # Read only required columns and filter for customer_id\n",
    "                try:\n",
    "                    # First, check if customer_id exists in this file\n",
    "                    customer_check = pd.read_csv(\n",
    "                        file_path, \n",
    "                        usecols=['CUSTOMER_ID'], \n",
    "                        dtype={'CUSTOMER_ID': int}\n",
    "                    )\n",
    "    \n",
    "                    if customer_id in customer_check['CUSTOMER_ID'].values:\n",
    "                        # Read only required columns with appropriate dtypes\n",
    "                        df = pd.read_csv(\n",
    "                            file_path,\n",
    "                            usecols=required_columns,\n",
    "                            dtype={\n",
    "                                'CUSTOMER_ID': int,\n",
    "                                'TERMINAL_ID': int,\n",
    "                                'TX_AMOUNT': float,\n",
    "                                'TX_TIME_SECONDS': float,\n",
    "                                'TX_TIME_DAYS': float\n",
    "                            },\n",
    "                            parse_dates=['TX_DATETIME']\n",
    "                        )\n",
    "                        \n",
    "                        df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'], errors='coerce')\n",
    "                        df = df.dropna(subset=['TX_DATETIME'])\n",
    "\n",
    "                        # Filter for specific customer\n",
    "                        customer_df = df[df['CUSTOMER_ID'] == customer_id]\n",
    "                        \n",
    "                        if not customer_df.empty:\n",
    "                            customer_data.append(customer_df)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {filename}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        if not customer_found:\n",
    "            raise ValueError(f\"Customer ID {customer_id} not found in transaction history\")\n",
    "\n",
    "        if not customer_data:\n",
    "            # Return empty DataFrame with correct columns if no data found\n",
    "            return pd.DataFrame(columns=required_columns)\n",
    "        \n",
    "        # Combine all customer data and sort by datetime\n",
    "        customer_history = pd.concat(customer_data, ignore_index=True)\n",
    "        return customer_history.sort_values('TX_DATETIME')    \n",
    "    \n",
    "\n",
    "    def _calculate_customer_amount_features(self, customer_txns: pd.DataFrame, \n",
    "                                         current_amount: float,\n",
    "                                         tx_datetime: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Calculate amount features based on customer's transaction history\"\"\"\n",
    "\n",
    "        # Filter for transactions before current timestamp\n",
    "        past_txns = customer_txns[customer_txns['TX_DATETIME'] < tx_datetime]\n",
    "        \n",
    "        if past_txns.empty:\n",
    "            return {\n",
    "                'amount': current_amount,\n",
    "                'amount_log': np.log1p(current_amount),\n",
    "                'amount_rounded': round(current_amount, -1),\n",
    "                'is_round_amount': 1 if current_amount % 10 == 0 else 0,\n",
    "                'amount_mean': current_amount,\n",
    "                'amount_std': 0.1 * current_amount,\n",
    "                'amount_max': current_amount,\n",
    "                'amount_min': current_amount,\n",
    "                'amount_deviation': 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate amount statistics\n",
    "        amount_stats = past_txns['TX_AMOUNT'].agg(['mean', 'std', 'max', 'min'])\n",
    "        \n",
    "        # Handle std=0 case\n",
    "        if pd.isna(amount_stats['std']) or amount_stats['std'] == 0:\n",
    "            amount_stats['std'] = 0.1 * amount_stats['mean']\n",
    "            \n",
    "        amount_deviation = float(abs(current_amount - amount_stats['mean']) / amount_stats['std'])\n",
    "        \n",
    "        return {\n",
    "            'amount': float(current_amount),\n",
    "            'amount_log': float(np.log1p(current_amount)),\n",
    "            'amount_rounded': float(round(current_amount, -1)),\n",
    "            'is_round_amount':float(1 if current_amount % 10 == 0 else 0),\n",
    "            'amount_mean': float(amount_stats['mean']),\n",
    "            'amount_std': float(amount_stats['std']),\n",
    "            'amount_max': float(amount_stats['max']),\n",
    "            'amount_min': float(amount_stats['min']),\n",
    "            'amount_deviation': amount_deviation\n",
    "        }\n",
    "\n",
    "    def _calculate_sequence_features(self, customer_txns: pd.DataFrame,\n",
    "                                  terminal_id: int, amount: float,\n",
    "                                  tx_datetime: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Calculate sequence features from customer transaction history\"\"\"\n",
    "\n",
    "        customer_txns['TX_DATETIME'] = pd.to_datetime(customer_txns['TX_DATETIME'], errors='coerce')\n",
    "\n",
    "        past_txns = customer_txns[customer_txns['TX_DATETIME'] < tx_datetime]\n",
    "        \n",
    "        if past_txns.empty:\n",
    "            return {\n",
    "                'time_since_last': 86400,\n",
    "                'time_until_next': 86400,\n",
    "                'amount_diff_last': 0,\n",
    "                'amount_diff_next': 0,\n",
    "                'terminal_changed': 1,\n",
    "                'tx_velocity_1h': 0,\n",
    "                'tx_velocity_24h': 0,\n",
    "                'amount_velocity_1h': 0,\n",
    "                'amount_velocity_24h': 0,\n",
    "                'unique_terminals_24h': 0,\n",
    "                'repeated_terminal': 0\n",
    "            }\n",
    "        \n",
    "        # Time windows\n",
    "        one_hour_ago = tx_datetime - timedelta(hours=1)\n",
    "        one_day_ago = tx_datetime - timedelta(days=1)\n",
    "        \n",
    "        # Get last transaction\n",
    "        last_tx = past_txns.iloc[-1]\n",
    "        \n",
    "        # Calculate time-based features\n",
    "        time_since_last = (tx_datetime - last_tx['TX_DATETIME']).total_seconds()\n",
    "        amount_diff = amount - last_tx['TX_AMOUNT']\n",
    "        terminal_changed = 1 if last_tx['TERMINAL_ID'] != terminal_id else 0\n",
    "        \n",
    "        # Calculate velocity features using vectorized operations\n",
    "        mask_1h = (past_txns['TX_DATETIME'] > one_hour_ago)\n",
    "        mask_24h = (past_txns['TX_DATETIME'] > one_day_ago)\n",
    "        \n",
    "        txns_1h = past_txns[mask_1h]\n",
    "        txns_24h = past_txns[mask_24h]\n",
    "        \n",
    "        return {\n",
    "            'time_since_last': time_since_last,\n",
    "            'time_until_next': 0,\n",
    "            'amount_diff_last': amount_diff,\n",
    "            'amount_diff_next': 0,\n",
    "            'terminal_changed': terminal_changed,\n",
    "            'tx_velocity_1h': len(txns_1h),\n",
    "            'tx_velocity_24h': len(txns_24h),\n",
    "            'amount_velocity_1h': txns_1h['TX_AMOUNT'].sum(),\n",
    "            'amount_velocity_24h': txns_24h['TX_AMOUNT'].sum(),\n",
    "            'unique_terminals_24h': txns_24h['TERMINAL_ID'].nunique(),\n",
    "            'repeated_terminal': past_txns[past_txns['TERMINAL_ID'] == terminal_id].shape[0]\n",
    "        }\n",
    "\n",
    "    def _calculate_terminal_features(self, terminal_id: int, tx_datetime: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate terminal behavior features from historical data\n",
    "        \"\"\"\n",
    "        # Load terminal transactions\n",
    "        terminal_txns = []\n",
    "        required_columns = ['TERMINAL_ID', 'TX_AMOUNT', 'TX_DATETIME', 'TX_FRAUD']\n",
    "        \n",
    "        for filename in os.listdir(self.data_directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.data_directory, filename)\n",
    "                \n",
    "                # First check if terminal exists in file\n",
    "                terminal_check = pd.read_csv(\n",
    "                    file_path,\n",
    "                    usecols=['TERMINAL_ID'],\n",
    "                    dtype={'TERMINAL_ID': int}\n",
    "                )\n",
    "                \n",
    "                if terminal_id in terminal_check['TERMINAL_ID'].values:\n",
    "                    df = pd.read_csv(\n",
    "                        file_path,\n",
    "                        usecols=required_columns,\n",
    "                        dtype={\n",
    "                            'TERMINAL_ID': int,\n",
    "                            'TX_AMOUNT': float,\n",
    "                            'TX_FRAUD': int\n",
    "                        },\n",
    "                        parse_dates=['TX_DATETIME']\n",
    "                    )\n",
    "                    \n",
    "                    terminal_df = df[df['TERMINAL_ID'] == terminal_id]\n",
    "                    if not terminal_df.empty:\n",
    "                        terminal_txns.append(terminal_df)\n",
    "        \n",
    "        if not terminal_txns:\n",
    "            # Return default values if no history found\n",
    "            return {\n",
    "                'terminal_tx_count': 1,\n",
    "                'terminal_amount_mean': 0,\n",
    "                'terminal_amount_std': 0,\n",
    "                'terminal_amount_median': 0,\n",
    "                'terminal_tx_count_large': 0,\n",
    "                'terminal_tx_time_mean': 86400,  # 1 day in seconds\n",
    "                'terminal_tx_time_std': 3600,    # 1 hour in seconds\n",
    "                'terminal_fraud_rate_smoothed': 0.01\n",
    "            }\n",
    "        \n",
    "        # Combine all terminal transactions and filter for past transactions\n",
    "        terminal_history = pd.concat(terminal_txns, ignore_index=True)\n",
    "        past_txns = terminal_history[terminal_history['TX_DATETIME'] < tx_datetime]\n",
    "        \n",
    "        if past_txns.empty:\n",
    "            return {\n",
    "                'terminal_tx_count': 1,\n",
    "                'terminal_amount_mean': 0,\n",
    "                'terminal_amount_std': 0,\n",
    "                'terminal_amount_median': 0,\n",
    "                'terminal_tx_count_large': 0,\n",
    "                'terminal_tx_time_mean': 86400,\n",
    "                'terminal_tx_time_std': 3600,\n",
    "                'terminal_fraud_rate_smoothed': 0.01\n",
    "            }\n",
    "        \n",
    "        # Calculate amount statistics\n",
    "        amount_stats = past_txns['TX_AMOUNT'].agg(['mean', 'std', 'median'])\n",
    "        \n",
    "        # Calculate large transaction count (transactions above mean)\n",
    "        large_tx_count = (past_txns['TX_AMOUNT'] > amount_stats['mean']).sum()\n",
    "        \n",
    "        # Calculate time between transactions\n",
    "        past_txns_sorted = past_txns.sort_values('TX_DATETIME')\n",
    "        time_diffs = past_txns_sorted['TX_DATETIME'].diff().dt.total_seconds()\n",
    "        time_mean = time_diffs.mean() if len(time_diffs) > 1 else 86400\n",
    "        time_std = time_diffs.std() if len(time_diffs) > 1 else 3600\n",
    "        \n",
    "        # Calculate fraud rate with Laplace smoothing\n",
    "        total_txns = len(past_txns)\n",
    "        fraud_count = past_txns['TX_FRAUD'].sum()\n",
    "        alpha = 0.01  # smoothing parameter\n",
    "        fraud_rate_smoothed = (fraud_count + alpha) / (total_txns + 2 * alpha)\n",
    "        \n",
    "        return {\n",
    "            'terminal_tx_count': total_txns,\n",
    "            'terminal_amount_mean': amount_stats['mean'],\n",
    "            'terminal_amount_std': amount_stats['std'] if pd.notnull(amount_stats['std']) else 0,\n",
    "            'terminal_amount_median': amount_stats['median'],\n",
    "            'terminal_tx_count_large': large_tx_count,\n",
    "            'terminal_tx_time_mean': time_mean,\n",
    "            'terminal_tx_time_std': time_std,\n",
    "            'terminal_fraud_rate_smoothed': fraud_rate_smoothed\n",
    "        }\n",
    "\n",
    "    def _preprocess_input(self, data):\n",
    "        \"\"\"Preprocess input with efficient customer-level feature calculation\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            if 'inputs' in data:\n",
    "                data = data['inputs']\n",
    "            if isinstance(data, dict):\n",
    "                data = pd.DataFrame([data])\n",
    "        \n",
    "        # Extract base values\n",
    "        customer_id = int(data['CUSTOMER_ID'].iloc[0])\n",
    "        terminal_id = int(data['TERMINAL_ID'].iloc[0])\n",
    "        amount = float(data['TX_AMOUNT'].iloc[0])\n",
    "        tx_datetime = pd.to_datetime(data['TX_DATETIME'].iloc[0])\n",
    "        \n",
    "        # Load customer's transaction history\n",
    "        customer_txns = self._load_customer_transactions(customer_id)\n",
    "        \n",
    "        # Calculate features using the loaded history\n",
    "        amount_features = self._calculate_customer_amount_features(\n",
    "            customer_txns, amount, tx_datetime\n",
    "        )\n",
    "        \n",
    "        sequence_features = self._calculate_sequence_features(\n",
    "            customer_txns, terminal_id, amount, tx_datetime\n",
    "        )\n",
    "        \n",
    "        terminal_features = self._calculate_terminal_features(\n",
    "            terminal_id, tx_datetime\n",
    "        )\n",
    "\n",
    "        # Create features DataFrame\n",
    "        features = pd.DataFrame(index=[0])\n",
    "        \n",
    "        # Map all features\n",
    "        features['feature_1'] = customer_id / 10000\n",
    "        features['feature_2'] = float(data['TX_TIME_SECONDS'].iloc[0]) / 86400\n",
    "        features['feature_3'] = float(data['TX_TIME_DAYS'].iloc[0]) / 7\n",
    "        features['feature_4'] = terminal_id / 1000\n",
    "        features['feature_5'] = amount_features['amount']\n",
    "        features['feature_6'] = amount_features['amount_log']\n",
    "        \n",
    "        # Temporal features\n",
    "        features['feature_7'] = tx_datetime.hour / 24\n",
    "        features['feature_8'] = tx_datetime.dayofweek / 7\n",
    "        features['feature_9'] = tx_datetime.month / 12\n",
    "        features['feature_10'] = 1 if tx_datetime.dayofweek >= 5 else 0\n",
    "        features['feature_11'] = 1 if (tx_datetime.hour >= 23 or tx_datetime.hour <= 4) else 0\n",
    "        features['feature_12'] = 1 if (8 <= tx_datetime.hour <= 10 or \n",
    "                                     16 <= tx_datetime.hour <= 18) else 0\n",
    "        \n",
    "        # Amount features\n",
    "        features['feature_13'] = amount_features['amount_deviation']\n",
    "        features['feature_14'] = amount_features['amount_mean']\n",
    "        features['feature_15'] = amount_features['amount_std']\n",
    "        features['feature_16'] = amount_features['amount_max']\n",
    "        features['feature_17'] = amount_features['amount_min']\n",
    "        \n",
    "        # Customer behavior features\n",
    "        features['feature_18'] = len(customer_txns)\n",
    "        features['feature_19'] = customer_txns['TERMINAL_ID'].nunique()\n",
    "        features['feature_20'] = customer_txns['TX_DATETIME'].dt.hour.mean()\n",
    "        features['feature_21'] = customer_txns['TX_DATETIME'].dt.hour.std()\n",
    "        \n",
    "        # Terminal features\n",
    "        features['feature_22'] = terminal_features['terminal_tx_count']\n",
    "        features['feature_23'] = terminal_features['terminal_amount_mean']\n",
    "        features['feature_24'] = terminal_features['terminal_amount_std']\n",
    "        features['feature_25'] = terminal_features['terminal_amount_median']\n",
    "        features['feature_26'] = terminal_features['terminal_tx_count_large']\n",
    "        features['feature_27'] = terminal_features['terminal_tx_time_mean']\n",
    "        features['feature_28'] = terminal_features['terminal_tx_time_std']\n",
    "        features['feature_29'] = terminal_features['terminal_fraud_rate_smoothed']\n",
    "        \n",
    "        # Sequence features\n",
    "        features['feature_30'] = sequence_features['time_since_last']\n",
    "        features['feature_31'] = sequence_features['time_until_next']\n",
    "        features['feature_32'] = sequence_features['amount_diff_last']\n",
    "        features['feature_33'] = sequence_features['amount_diff_next']\n",
    "        features['feature_34'] = sequence_features['terminal_changed']\n",
    "        features['feature_35'] = sequence_features['tx_velocity_1h']\n",
    "        features['feature_36'] = sequence_features['tx_velocity_24h']\n",
    "        features['feature_37'] = sequence_features['repeated_terminal']\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _load_model(self, workspace, model_name, version):\n",
    "        return mlflow.pyfunc.load_model(\n",
    "        model_uri=f\"models:/{model_name}/{version}\"\n",
    "    )\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load models\"\"\"\n",
    "        if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "            mlflow.set_tracking_uri(\"databricks\")\n",
    "        else:\n",
    "            mlflow.set_tracking_uri(\"local\")\n",
    "            \n",
    "        self.xgb_model = self._load_model(context, 'xgb_model', self.model_versions['xgb_model'])\n",
    "        self.rf_model = self._load_model(context, 'rf_model', self.model_versions['rf_model'])\n",
    "        self.nn_model = self._load_model(context, 'pytorch_model', self.model_versions['pytorch_model'])\n",
    "        \n",
    "        self.feature_names = [f'feature_{i}' for i in range(1, 38)]\n",
    "\n",
    "    def _get_probabilities(self, model, X, model_type):\n",
    "        \"\"\"Get probability predictions from a model with consistent output format\"\"\"\n",
    "        try:\n",
    "            if model_type in ['xgb', 'rf']:\n",
    "                # For sklearn-based models (XGBoost and Random Forest)\n",
    "                probs = model.predict_proba(X)\n",
    "                return np.array(probs)  # Returns array of shape (n_samples, 2)\n",
    "                \n",
    "            elif model_type == 'nn':\n",
    "                # For PyTorch model\n",
    "                X_tensor = torch.FloatTensor(X.values)\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    outputs = model(X_tensor)\n",
    "                    # Ensure output has both class probabilities\n",
    "                    if outputs.shape[1] == 1:\n",
    "                        pos_probs = outputs.numpy()\n",
    "                        neg_probs = 1 - pos_probs\n",
    "                        probs = np.column_stack([neg_probs, pos_probs])\n",
    "                    else:\n",
    "                        probs = outputs.numpy()\n",
    "                return probs\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error getting probabilities for {model_type}: {str(e)}\")\n",
    "\n",
    "    def predict(self, context, input_data):\n",
    "        \"\"\"Make predictions using the ensemble with detailed probability outputs\"\"\"\n",
    "        try:\n",
    "            # Preprocess input data\n",
    "            X = self._preprocess_input(input_data)\n",
    "            \n",
    "            # Get individual model probabilities (each returns shape (n_samples, 2))\n",
    "            xgb_probs = self._get_probabilities(self.xgb_model, X, 'xgb')\n",
    "            rf_probs = self._get_probabilities(self.rf_model, X, 'rf')\n",
    "            nn_probs = self._get_probabilities(self.nn_model, X, 'nn')\n",
    "            \n",
    "            results = []\n",
    "            for i in range(len(X)):\n",
    "                # Get individual model probabilities for both classes\n",
    "                xgb_prob_pair = xgb_probs[i]\n",
    "                rf_prob_pair = rf_probs[i]\n",
    "                nn_prob_pair = nn_probs[i]\n",
    "                \n",
    "                # Calculate weighted ensemble probabilities\n",
    "                ensemble_probs = np.zeros(2)\n",
    "                for j in range(2):\n",
    "                    ensemble_probs[j] = (\n",
    "                        self.weights[0] * xgb_prob_pair[j] +\n",
    "                        self.weights[1] * rf_prob_pair[j] +\n",
    "                        self.weights[2] * nn_prob_pair[j]\n",
    "                    )\n",
    "                \n",
    "                # Format probability outputs\n",
    "                result = {\n",
    "                    'probabilities': {\n",
    "                        'ensemble': {\n",
    "                            'non_fraud': f\"{ensemble_probs[0]:.4f}\",\n",
    "                            'fraud': f\"{ensemble_probs[1]:.4f}\"\n",
    "                        },\n",
    "                        'models': {\n",
    "                            'xgboost': {\n",
    "                                'non_fraud': f\"{xgb_prob_pair[0]:.4f}\",\n",
    "                                'fraud': f\"{xgb_prob_pair[1]:.4f}\"\n",
    "                            },\n",
    "                            'random_forest': {\n",
    "                                'non_fraud': f\"{rf_prob_pair[0]:.4f}\",\n",
    "                                'fraud': f\"{rf_prob_pair[1]:.4f}\"\n",
    "                            },\n",
    "                            'neural_network': {\n",
    "                                'non_fraud': f\"{nn_prob_pair[0]:.4f}\",\n",
    "                                'fraud': f\"{nn_prob_pair[1]:.4f}\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    'prediction': {\n",
    "                        'label': \"FRAUD\" if ensemble_probs[1] >= 0.5 else \"NON-FRAUD\",\n",
    "                        'confidence': f\"{max(ensemble_probs):.4f}\"\n",
    "                    },\n",
    "                    'model_agreement': {\n",
    "                        'count': len([p for p in [xgb_prob_pair[1], rf_prob_pair[1], nn_prob_pair[1]] \n",
    "                                    if (p >= 0.5) == (ensemble_probs[1] >= 0.5)]),\n",
    "                        'ratio': f\"{len([p for p in [xgb_prob_pair[1], rf_prob_pair[1], nn_prob_pair[1]] if (p >= 0.5) == (ensemble_probs[1] >= 0.5)]) / 3:.2f}\"\n",
    "                    }\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Prediction error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd292b7-7be6-42d4-a343-33174f60e628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import pandas as pd\n",
    "serving_payload = {\n",
    "        \"TRANSACTION_ID\": 4781,\n",
    "        \"TX_DATETIME\": \"2024-10-29 05:57:40\",\n",
    "        \"CUSTOMER_ID\": 596,\n",
    "        \"TERMINAL_ID\": 139,\n",
    "        \"TX_AMOUNT\": 251.25,\n",
    "        \"TX_TIME_SECONDS\": 21460,\n",
    "        \"TX_TIME_DAYS\": 29\n",
    "    }\n",
    "\n",
    "# a = pd.DataFrame(serving_payload)\n",
    "# a\n",
    "\n",
    "model_versions = {\n",
    "    'xgb_model': '1',\n",
    "    'rf_model': '1',\n",
    "    'pytorch_model': '1'\n",
    "}\n",
    "\n",
    "model = FraudDetectionEnsemble(\n",
    "    model_versions=model_versions,\n",
    "    data_directory='/Workspace/Users/kehinde.awomuti@pwc.com/ccfrauddetection/data'\n",
    ")\n",
    "\n",
    "data = model._preprocess_input(serving_payload)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba723d3e-8237-4273-9295-88c80b5aae04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/2cec9d32cdbb44f39f217c61bd8cac57/xgboost_model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.sklearn.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict_proba(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a644840-c0bb-4ef4-b352-025a155d540c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/2cec9d32cdbb44f39f217c61bd8cac57/random_forest_model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.sklearn.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict_proba(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1cc5ddc-0958-4a40-8307-10a186213f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/50d0e65c043b4c5390e7608a8d0ca98a/pytorch_model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(pd.DataFrame(data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f542234c-d5fc-4752-b959-27a213260beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import validate_serving_input\n",
    "\n",
    "model_uri = 'runs:/082995c105bf418fbffbd9135871d348/fraud_detection_inference'\n",
    "\n",
    "# The model is logged with an input example. MLflow converts\n",
    "# it into the serving payload format for the deployed model endpoint,\n",
    "# and saves it to 'serving_input_payload.json'\n",
    "serving_payload = \"\"\"{\n",
    "  \"inputs\": {\n",
    "    \"TRANSACTION_ID\": 4781,\n",
    "    \"TX_DATETIME\": \"2024-10-29 05:57:40\",\n",
    "    \"CUSTOMER_ID\": 596,\n",
    "    \"TERMINAL_ID\": 139,\n",
    "    \"TX_AMOUNT\": 251.25,\n",
    "    \"TX_TIME_SECONDS\": 21460,\n",
    "    \"TX_TIME_DAYS\": 29\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "# Validate the serving payload works on the model\n",
    "validate_serving_input(model_uri, serving_payload)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
